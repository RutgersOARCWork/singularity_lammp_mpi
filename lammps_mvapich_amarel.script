#!/bin/bash

#SBATCH --clusters=amarel            # Select which system(s) to use
#SBATCH --partition=oarc             # Partition (job queue)
###SBATCH --account=genral
#SBATCH --requeue                    # Return job to the queue if preempted
#SBATCH --job-name=LAMMPSX           # Assign an short name to your job
#SBATCH --nodes=2                    # Number of nodes you require
#SBATCH --ntasks=24                  # Total # of tasks across all nodes
#SBATCH --cpus-per-task=1            # Cores per task (>1 if multithread tasks)
#SBATCH --mem=5G                     # Real memory (RAM) required per node
#SBATCH --time=10:00                 # Total run time limit (DD-HH:MM:SS)
#SBATCH --output=slurm.%N.%j.out     # STDOUT file for SLURM output

## Environment settings needed for this job
module purge
module load singularity/3.6.4 mvapich2/2.2


## Run the job
srun  --mpi=pmi2 singularity run lammps_mvapich_v1.sif < in.binary

## Capture job accounting info (OPTIONAL)
sleep 10
echo ""
sacct --units=G --format=MaxRSS,MaxDiskRead,MaxDiskWrite,Elapsed,NodeList -j $SLURM_JOBID | sed -n -e 1,2p -e 5p
echo ""
echo -n "Fair-share score: ";sshare | grep $USER | grep general | awk '{print $NF}'
echo ""
sleep 2

